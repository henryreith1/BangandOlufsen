---
title: "BangAndOlufsen"
author: "Henry Reith, Terence Carey, Bailey Williamson"
date: "November 20, 2018"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
#Introduction
###The purpose of this project is the utilize a data-set from the Google App Store (hosted on Kaggle) to determine the most important predictors of the success of a mobile application. We define a successful mobile application as having a relatively high number of installs. This data-set contains the following categories: App, Category, Rating, Reviews, Size, Installs, Type, Price, Content Rating, and Updates.
###1. Can you even predict if an app will be successful given these categories?
###2. If so, what features are most important in predicting the relative success of a mobile application?
###3. What genre of mobile application is most likely to be successful?
###The results of these analyses could be useful for app developers attempting to develop a successful mobile application. A person investing capital (who wants to maximize their return) in the development of such an application could use these results to determine what type of application will be most successful.


```{r, include=FALSE}
library(ggplot2)
library(cluster)
library(caret)
library(rpart)
library(ggthemes)
library(randomForest)

```

#Cleaning Data 
###Got rid of unconsistencies, NA's, and items that we did not feel were useful. Turned installs from a numeric to factor. Added new column that put the number of updates into tiers and removed apps that had a much higher number of installations  than the others. Also creates a separate data frame that still has the outliers, to show the significance of them later. 
```{r, echo=FALSE, include=FALSE}
d <- read.csv("googleplaystore.csv")
d <- d[d$Size != 'Varies with device',]
d <- d[d$Current.Ver != 'Varies with device',]
d <- d[!is.nan(d$Current.Ver),]
d <- d[!is.nan(d$Rating),]
d <- d[!is.nan(d$Price),]
d <- d[!is.nan(d$Type),]
d$Installs <- gsub(',', '', d$Installs)
d$Installs <- gsub('\\+', '', d$Installs)
d$Installs <- as.numeric(as.character((d$Installs)))
d$Current.Ver <- gsub('[a-z_()A-Z\\-\\+]', '', d$Current.Ver)
d$Price <- gsub('\\$', '', d$Price)
d$Price <- as.numeric(as.character((d$Price)))
d$Updates <- ifelse(d$Current.Ver >= 2 , ifelse(d$Current.Ver >= 4, 2, 1), 0)
d <- subset(d, select = c('App','Category','Rating','Reviews','Size','Installs','Type','Price','Content.Rating','Last.Updated','Updates','Android.Ver' ))
d <- d[order(d$Installs, decreasing = TRUE),]

d=na.omit(d)
doutliers <- d
d <- tail(d, nrow(d) -34)

```


#Subsetting our data and aggregating so that we can reorder for the barplots 
```{r}

yz <- subset(d, select =c('Category', 'Installs'))
yz<-aggregate(yz$Installs, by=list(Category=yz$Category), FUN=sum)

xz <- subset(d, select =c('Category', 'Rating'))
xz<-aggregate(xz$Rating, by=list(Category=xz$Category), FUN=mean)


```


#Bar Plots
```{r, echo=FALSE}



g <- ggplot(xz, aes(reorder(Category,x),x))
g2 <- ggplot(yz, aes(reorder(Category,x),x))

g + geom_bar(stat = "identity") + coord_flip() + ggtitle("Category By Rating")
g2 + geom_bar(stat = "identity") + coord_flip()+ ggtitle("Category By Installs")
```

#Random Forest Classifier 
###Created a random forest classifier to predict which features determined Installs. We were succesful in building the model but could not generate a confusion matrix. Attempts for confusion matrix are commented out in code. This was a successful model with a 75% Variables explained.  This is thus the best model that we have.
```{r, echo=FALSE}


set.seed(100)
train <- sample(nrow(d), 0.7*nrow(d), replace = FALSE)
TrainSet <- d[train,]
ValidSet <- d[-train,]


model1 <- randomForest(Installs ~ Category +Type + Rating + Reviews , data = TrainSet, nTrees = 300, mtry = 3, importance = TRUE, savePredictions = "final", classProbs = TRUE)

model1

```

```{r}

preds <- predict(model1, ValidSet)
head(preds)

actuals <- predict(model1, TrainSet)
head(actuals)


plot(preds, ValidSet$Installs) + abline(preds,actuals) + abline(0,1, col='red') 
```


```{r}
imp <- importance(model1)
imp

```


```{r}
model2 <- randomForest(Installs ~ Category +Type + Rating + Reviews + Price + Content.Rating + Updates , data = TrainSet, nTrees = 500, mtry = 3, importance = TRUE, savePredictions = "final", classProbs = TRUE)


model2



```

```{r}
imp2 <- importance(model2)
imp2
FeatureImportance <- as.data.frame(imp2)
names(FeatureImportance) <- c("IncMSE", "IncNodePurity")
FeatureImportance <- FeatureImportance[order(FeatureImportance$IncMSE, decreasing =  TRUE),]
FeatureImportance

```

```{r}

preds2 <- predict(model2, ValidSet)
head(preds)

actuals2 <- predict(model2, TrainSet)
head(actuals)


plot(preds2, ValidSet$Installs) + abline(preds2,actuals2) + abline(0,1, col='red') 

ValidSet$preds <- preds2

write.table(ValidSet, file = 'validset.csv', sep = ",", row.names = F)
```


#RPart Model Before Removal of Outliers
###Created RPart model with RSquared value of .4
```{r, echo=FALSE}
set.seed(4000)
dmv = dummyVars(~Category + Reviews + Rating + Size + Type + Price + Content.Rating + Last.Updated + Updates + Android.Ver , data = doutliers)
features = predict(dmv, doutliers)
y = doutliers$Installs

ctr <- trainControl(method = "cv", number = 4, classProbs = F, search='random')
fit = train(features,y,
            method = 'rpart',
            trControl = ctr, tuneLength = 10)
fit


```

#RPart Unsuccessful
###Created RPart model with RSquared Value of .07. This model did not include our category of Reviews which shows the importance of this variable. 
```{r, echo=FALSE}

d <- na.omit(d)

dmv = dummyVars(~Category + Rating , data = d)
features = predict(dmv, d)
y = d$Installs

ctr <- trainControl(method = "cv", number = 4, classProbs = F, search='random')
fit = train(features,y,
            method = 'rpart',
            trControl = ctr, tuneLength = 10)
fit
```

#After Removal of Outliers
###RPart model with RSquared value of .69. This model was created after removing outliers and includes Reviews as a factor. 
```{r, echo=FALSE}
set.seed(4000)
dmv = dummyVars(~Category + Reviews + Rating, data = d)
features = predict(dmv,d)
y = d$Installs

ctr <- trainControl(method = "cv", number = 4, classProbs = F, search = "random")
fit <- train(features,y , method = 'rpart', trControl = ctr, tuneLength = 10)
fit

```

#RPart Unsuccessful
###Created RPart model with RSquared Value of .07. This model did not include our category of Reviews which shows the importance of this variable. 
```{r, echo=FALSE}

dmv = dummyVars(~Category + Rating , data = d)
features = predict(dmv, d)
y = d$Installs

ctr <- trainControl(method = "cv", number = 4, classProbs = F, search='random')
fit = train(features,y,
            method = 'rpart',
            trControl = ctr, tuneLength = 10)
fit
```

#Clustering
###These 3 cluster models attempt to cluster ratings with various other categories in our dataset. We found little predictive capability from these models. The only potentially useful clustered result in the Ratings vs. Installs, where a clear correlation is visible in the cluster plot. 

```{r}
#Initialize random variable
set.seed(30)
d$Reviews <- as.numeric(d$Reviews)

d1<- subset(d, select=c("Installs", "Rating"))
clusters2<- kmeans(scale(d1), 4, nstart=25)
d1$cluster=as.factor(clusters2$cluster)
ggplot(d1, aes(x=Rating, y=Installs, color=cluster)) +geom_point()


d2<- subset(d, select=c("Installs", "Updates"))
clusters3<- kmeans(scale(d2), 4, nstart=25)
d2$cluster=as.factor(clusters3$cluster)
ggplot(d2, aes(x=Updates, y=Installs, color=cluster)) +geom_point()

d3<- subset(d, select=c("Installs", "Reviews"))
clusters4<- kmeans(scale(d3), 4, nstart=25)
d3$cluster=as.factor(clusters4$cluster)
ggplot(d3, aes(x=Reviews, y=Installs, color=cluster)) +geom_point()
```


#Executive Summary
The objective of this project was to determine which characteristics contribute to a successful mobile application. Data for this project was sourced from a Google Play Store database hosted on Kaggle. This data was then cleaned by removing unnecessary columns and removing problematic data entries (e.g. NaN values). A Random Forest classifier scheme was utilized to classify which categories in our data-set were most important in predicting a successful mobile application. From this scheme, it was determined that "Reviews", "Rating", and "Category" were the most important categories. An R-part method was utilized in R to try to predict what resulted in a high number of installs and it had a R-squared . K-means clustering was used to determine the following relationships: "Rating" vs. "Installs", "Rating" vs. "Reviews", and "Rating" vs. "Updates". The K-means analyses were ultimately not useful for predicting a successful app, since there were little to no useful clusters produced.. However, a weak positive correlation was noted for the "Rating" vs. "Installs" relationship. Ultimately, the most useful results were produced by the Random Forest analysis of our data, yielding an R-squared value of 0.77.