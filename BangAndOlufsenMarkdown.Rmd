---
title: "BangAndOlufsen"
author: "Henry Reith"
date: "November 20, 2018"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
###Cleaning Data 
#Got rid of unconsistencies, NA's, and items that we did not feel were useful. Turned installs from a numeric to factor. Added new column that put the number of updates into tiers and removed things that had a much higher of installs than others.  
```{r, echo=FALSE}

library(caret)
library(rpart)
d <- read.csv("googleplaystore.csv")
d <- d[d$Size != 'Varies with device',]
d <- d[d$Current.Ver != 'Varies with device',]
d <- d[!is.nan(d$Current.Ver),]
d <- d[!is.nan(d$Rating),]
d <- d[!is.nan(d$Price),]
d <- d[!is.nan(d$Type),]
d$Installs <- gsub(',', '', d$Installs)
d$Installs <- gsub('\\+', '', d$Installs)
d$Installs <- as.numeric(as.character((d$Installs)))
d$Current.Ver <- gsub('[a-z_()A-Z\\-\\+]', '', d$Current.Ver)
d$Price <- gsub('\\$', '', d$Price)
d$Price <- as.numeric(as.character((d$Price)))
d$Updates <- ifelse(d$Current.Ver >= 2 , ifelse(d$Current.Ver >= 4, 2, 1), 0)
d <- subset(d, select = c('App','Category','Rating','Reviews','Size','Installs','Type','Price','Content.Rating','Last.Updated','Updates','Android.Ver' ))
d <- d[order(d$Installs, decreasing = TRUE),]
d <- tail(d, nrow(d) -34)

```
###Bar Plots
```{r, echo=FALSE}
library(ggplot2)
library(ggthemes)

g <- ggplot(d, aes(Category,Rating))
g2 <- ggplot(d, aes(Category,Installs))

g + geom_bar(stat = "identity") + coord_flip() + ggtitle("Category By Rating")
g2 + geom_bar(stat = "identity") + coord_flip()+ ggtitle("Category By Installs")
```

###Random Forest Classifier 
#Created a random forest classifier to predict which features determined Installs. We were succesful in building the model but could not generate a confusion matrix. Attempts for confusion matrix are commented out in code. This was a successful model with a 75% Variables explained.  This is thus the best model that we have.
```{r, echo=FALSE}
library(randomForest)
library(caret)


set.seed(100)
train <- sample(nrow(d), 0.7*nrow(d), replace = FALSE)
TrainSet <- d[train,]
ValidSet <- d[-train,]


model1 <- randomForest(Installs ~ Category +Type + Rating + Reviews , data = TrainSet, nTrees = 300, mtry = 3, importance = TRUE, savePredictions = "final", classProbs = TRUE)
model1

#model1$pred[order(model1$pred),2]

#class_log <- ifelse(model1[,1] > .5, "YES", "NO")
#p <- confusionMatrix(class_log, ValidSet[["Installs"]])
#p

#confusionMatrix(model1, conf.level = 0.95, threshold = 0.8)
#confusionMatrix(model1, ValidSet, positive = NULL, dnn = c("Prediction", "Reference"))

#p1 <- predict(model1, TrainSet, type="response")
#print("Prediction & Confusion Matrix - train data")

#confusionMatrix(model1$pred[order(model1$pred),2], TrainSet$Installs)
```



###RPart Model Before Removal of Outliers
#Created RPart model with RSquared value of .49
```{r, echo=FALSE}
library(caret)
d <- na.omit(d)

set.seed(4000)
dmv = dummyVars(~Category + Reviews + Rating + Size + Type + Price + Content.Rating + Last.Updated + Updates + Android.Ver , data = d)
features = predict(dmv, d)
#head(features)
y = d$Installs
#plot(density(y))

ctr <- trainControl(method = "cv", number = 4, classProbs = F, search='random')
fit = train(features,y,
            method = 'rpart',
            trControl = ctr, tuneLength = 10)
fit


```

###RPart Unsuccessful
#Created RPart model with RSquared Value of .07. This model did not include our category of Reviews which shows the importance of this variable. 
```{r, echo=FALSE}

library(caret)
d <- na.omit(d)

dmv = dummyVars(~Category + Rating , data = d)
features = predict(dmv, d)
#head(features)
y = d$Installs
#plot(density(y))

ctr <- trainControl(method = "cv", number = 4, classProbs = F, search='random')
fit = train(features,y,
            method = 'rpart',
            trControl = ctr, tuneLength = 10)
fit
```

###After Removal of Outliers
#RPart model with RSquared value of .69. This model was created after removing outliers and includes Reviews as a factor. 
```{r, echo=FALSE}
d <- na.omit(d)
set.seed(4000)
dmv = dummyVars(~Category + Reviews + Rating, data = d)
features = predict(dmv,d)
y = d$Installs

ctr <- trainControl(method = "cv", number = 4, classProbs = F, search = "random")
fit <- train(features,y , method = 'rpart', trControl = ctr, tuneLength = 10)
fit

```

###These 3 cluster models attempt to cluster ratings with various other categories in our dataset. We found little predictive capability from these models. The only potentially useful clustered result in the Ratings vs. Installs, where a clear correlation is visible in the cluster plot. 

```{r}
library(ggplot2)
library(cluster)

#Initialize random variable
set.seed(30)

d=na.omit(d) #omit NA values
d$Reviews <- as.numeric(d$Reviews)
d$Rating <- as.numeric(d$Rating)

clusters<-kmeans(scale(d[,4:3]), 5, nstart=25) 

d$cluster=as.factor(clusters$cluster)

ggplot(d, aes(x=Reviews, y=Rating, color=cluster)) +geom_point()

d1<- subset(d, select=c("Rating", "Installs"))
clusters2<- kmeans(scale(d1), 4, nstart=25)
d1$cluster=as.factor(clusters2$cluster)
ggplot(d1, aes(x=Installs, y=Rating, color=cluster)) +geom_point()


d2<- subset(d, select=c("Rating", "Updates"))
View(d2)
clusters3<- kmeans(scale(d2), 4, nstart=25)
d2$cluster=as.factor(clusters3$cluster)
ggplot(d2, aes(x=Updates, y=Rating, color=cluster)) +geom_point()
```